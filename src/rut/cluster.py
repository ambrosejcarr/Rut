import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import phenograph
from rut import sample, misc


# todo support plotting of results from cluster (i.e. confusion matrix)
class Cluster(sample.Sampled):

    def __init__(self, *args, **kwargs):
        self.cluster_labels_ = None
        self.confusion_matrix_ = None
        super().__init__(*args, **kwargs)

    @classmethod
    def cluster_map(cls, *args):
        """
        calculate phenograph community assignments across resampled data for each
        of the n observations in global variable data

        :param args: allows passing of arbitrary arguments necessary to evoke
          multiprocessing.Pool.map()
        :return np.ndarray: (n,) cluster assignments
        """
        complete_data = cls.get_shared_data()
        sample = cls._draw_sample(complete_data)
        pca = PCA(n_components=50, svd_solver='randomized')
        reduced = pca.fit_transform(sample)

        # cluster, omitting phenograph outputs
        with misc.suppress_stdout_stderr():
            clusters, *_ = phenograph.cluster(reduced, n_jobs=1)
        return clusters

    # todo consider building this as a sparse matrix (it probably is!)
    def cluster_reduce(self, results, save_confusion_matrix):
        """
        carry out consensus clustering across iterations of phenograph run on data,
        running k-means on the confusion matrix generated by these iterations with k fixed
        as the median number of communities selected by phenograph.

        :param list results: list of vectors of cluster labels
        :param bool save_confusion_matrix: if True, the confusion matrix generated across
          in-drop runs will be saved.
        :return np.ndarray: (n,) consensus cluster labels
        """
        # create confusion matrix
        n = self._index.shape[0]
        integrated = np.zeros((n, n), dtype=np.uint8)
        for i in np.arange(len(results)):
            labs = results[i]
            for j in np.arange(integrated.shape[0]):
                increment = np.equal(labs, labs[j]).astype(np.uint8)
                integrated[j, :] += increment

        if save_confusion_matrix:
            self.confusion_matrix_ = integrated

        # get mean cluster size
        k = int(np.round(np.mean([np.unique(r).shape[0] for r in results])))

        # cluster the integrated similarity matrix
        km = KMeans(n_clusters=15)
        metacluster_labels = km.fit_predict(integrated)

        # propagate ids back to original coordinates and return
        cluster_ids = np.ones(n, dtype=np.int) * -1
        cluster_ids[self._labels] = metacluster_labels
        return cluster_ids

    def fit(self, n_iter=10, n_processes=None, save_confusion_matrix=True):
        """fit consensus clustering across n_iter phenograph runs

        :param int n_iter: number of iterations of clustering to run
        :param int n_processes: number of processes to assign to clustering
        :param bool save_confusion_matrix: if True, save the confusion matrix generated
          by repeated phenograph runs
        :return np.array: self.cluster_labels_, consensus assignments across phenograph
          runs.
        """

        self.cluster_labels_ = self.run(
            n_iter=n_iter,
            fmap=self.cluster_map,
            freduce=self.cluster_reduce,
            n_processes=n_processes,
            freduce_kwargs=dict(save_confusion_matrix=save_confusion_matrix))

        return self.cluster_labels_
